{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b360b4",
   "metadata": {},
   "source": [
    "---\n",
    "# Review Based User Recommendations\n",
    "\n",
    "---\n",
    "## Problem Statement\n",
    "\n",
    "Context:\n",
    "\n",
    "As a Data Scientist, you’ve been asked to build a recommendation service for users on a vacation rental platform based on their previous experience.\n",
    "\n",
    "Task:\n",
    "\n",
    "Your task would be to develop a recommendation model that could recommend returning users new properties based on their old reviews. Let’s just assume that our platform has only vacation houses in London and we would like to recommend new properties only our returning users.\n",
    "\n",
    "Data:\n",
    "\n",
    "As an input you get London Airbnb Dataset where you can find user reviews and general information about listings.\n",
    "\n",
    "recommendations.zip\n",
    "\n",
    "Deliverables / outcome:\n",
    "\n",
    "Upon completion of your work, your presentation should encompass the following:\n",
    "* insights and challenges that you’ve faced during the discovery process;  \n",
    "* results of the sentiment analysis, how you extracted signals for the recommendation model;  \n",
    "* recommendation model itself: what approach and algorithm was selected, why and how it can be evaluated.  \n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "* New properties: \n",
    "    * using reviews as a proxy for bookings, a new property is taken as a property the reviewer has not reviewed\n",
    "    * in reality they may have booked a property and not reviewed it and this will be included in the new property recommendation\n",
    "* Returning users: \n",
    "    * taken to mean anyone who has made a review as they could all return to make another booking\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights & Challenges\n",
    "\n",
    "\n",
    "### User-Level Sparsity\n",
    "\n",
    "Only **~12% of users have reviewed more than one property** meaning most users only have 1 recorded interaction\n",
    "\n",
    "* Personalised modelling is therefore limited to a small subset of users\n",
    "* User preference must often be inferred from minimal history\n",
    "* Next-item prediction is inherently challenging for this user set\n",
    "\n",
    "This places greater importance on strong listing representations\n",
    "\n",
    "---\n",
    "\n",
    "### Long-Tail Listings-Review interactions\n",
    "\n",
    "The listing distribution is strongly long-tailed:\n",
    "\n",
    "* Most listings have few reviews\n",
    "* Only a small number accumulate high interaction counts\n",
    "\n",
    "As a result:\n",
    "\n",
    "* Popularity performs poorly at small K\n",
    "* The held-out listing is rarely globally popular\n",
    "* Absolute Recall@10 values are naturally low and must be interpreted in context\n",
    "\n",
    "---\n",
    "\n",
    "### Implicit Feedback Setting\n",
    "\n",
    "Reviews act as a proxy for bookings:\n",
    "\n",
    "* A review indicates interest\n",
    "* Absence of a review does not imply dislike\n",
    "\n",
    "This required:\n",
    "\n",
    "* Pairwise ranking (BPR) instead of regression\n",
    "* Careful negative sampling\n",
    "\n",
    "---\n",
    "\n",
    "### Impact of Feature Engineering\n",
    "\n",
    "Experiments showed that:\n",
    "\n",
    "* Structured features alone were insufficient\n",
    "* Hard negative sampling improved ranking sharpness\n",
    "* Adding summary text embeddings produced the most meaningful gains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e9f30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Reviews\n",
    "\n",
    "Only 12% of users have reviewed more than 1 property. This highlights significant user-level sparsity limiting how strongly personalised the model can be.\n",
    "\n",
    "## Sentiment\n",
    "\n",
    "A review indicates that a user selected and experienced a property. Regardless of whether the review is positive or negative, it reflects an initial preference for that type of listing.\n",
    "\n",
    "Given time constraints I used overall review sentiment as a soft preference signal rather than performing deeper aspect level extraction  \n",
    "* I convert the sentiment score into a smooth weight between 0 and 1  \n",
    "* A user profile becomes a weighted average of the listings they’ve reviewed  \n",
    "* Positive reviews contribute more strongly to the profile  \n",
    "* Negative reviews reduce influence but are not treated as strict dislikes  \n",
    "* This avoids overreacting to one-off bad experiences like a noisy weekend or a rude host that don’t necessarily reflect the user’s overall property preferences  \n",
    "\n",
    "### Sentiment distribution\n",
    "\n",
    "Review sentiment is highly skewed:  \n",
    "* ~91% positive  \n",
    "* ~6% negative  \n",
    "* ~2% neutral  \n",
    "\n",
    "Implications:  \n",
    "* Sentiment primarily acts as a strength of preference signal  \n",
    "* The weighting refines user profiles but does not drastically change ranking behaviour  \n",
    "\n",
    "### Further sentiment considerations for extended work\n",
    "\n",
    "The negative reviews don't necesarrily indicate that they were unhappy with their choice of property type and are often more situational, e.g:  \n",
    "* property specific - unclean, faulty appliances, noise, unresponsive host, small size  \n",
    "* location - hard to find, unsafe  \n",
    "\n",
    "A more advanced extension would involve:  \n",
    "* Extracting aspect level sentiment (e.g. location, host, cleanliness, noise)  \n",
    "* Aligning those aspects directly with listing features, e.g. if they mentioned location negatively this can be used alongside the location features  \n",
    "* Compare positive/negative topics against other reviews to recommend properties where consensus from reviews for these aspects align  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8893c9ed",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature Selection\n",
    "\n",
    "In order to select the features, I reviewed the data available with consideration to\n",
    "* what might be most informative to a recommendation based on review \n",
    "* practicality within time and computational constraints  \n",
    "\n",
    "## Structured Features\n",
    "\n",
    "Features that likely reflect what users actively consider when selecting vacation rentals\n",
    "\n",
    "* **Location based features**\n",
    "    * Neighbourhood\n",
    "        * strong indicator of location preference \n",
    "\n",
    "    * Latitude & Longitude\n",
    "        * capture nearby neighbourhoods\n",
    "\n",
    "* **Property based features**\n",
    "    * Room type\n",
    "        * entire home or a private room is likely to be one of the strongest drivers of booking decisions\n",
    "    * Property type\n",
    "        * apartment, house, loft etc. provide additional differentiation  \n",
    "    * Accommodates\n",
    "        * could be indicator of user type (solo, couple, group)\n",
    "    * Amenities\n",
    "        * property features such as wifi, kitchen, heating, washer, etc.  \n",
    "        * limited to the most frequent amenities to reduce sparsity and noise  \n",
    "\n",
    "* **Price**\n",
    "  * Although this is not necessarily the booking price as it is the price on scraping day it can be considered a proxy for property quality type\n",
    "\n",
    "## Text Features\n",
    "\n",
    "Capture the semantic representation not available within the structured features\n",
    "\n",
    "* **Summary**\n",
    "  * pre-trained sentence embeddings were used to encode listing summaries  \n",
    "  * capture qualitative differences such as style, ambience, and unique characteristics  \n",
    "\n",
    "## Sentiment\n",
    "\n",
    "Without time to break down sentiment into specific features and preferences, I used sentiment as a preference signal to avoid overfitting to one-off neagtive reivews\n",
    "\n",
    "* review sentiment was converted into a smooth weight between 0 and 1  \n",
    "* used to weight historical interactions when constructing user profiles  \n",
    "* negative reviews reduced influence rather than acting as strict negative labels  \n",
    "\n",
    "## Features Explicitly Excluded\n",
    "\n",
    "The following features were intentionally not used:\n",
    "\n",
    "* review score aggregates (rating, cleanliness score, etc.)  \n",
    "* number of reviews  \n",
    "* availability fields  \n",
    "\n",
    "Reasons:\n",
    "\n",
    "* they bias the model toward already popular listings rather than helping it learn individual user preferences\n",
    "* they are better suited for post-ranking adjustments or filtering (e.g. ensuring quality thresholds or availability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fbb059",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Model Architecture & Evaluation\n",
    "\n",
    "## Recommendation Model Approach\n",
    "\n",
    "Given the objective is to recommend new listings to returning users based on their past reviews, the model must infer user preferences from historical interactions.\n",
    "\n",
    "The model I chose took this approach\n",
    "* Learning a dense representation of each listing\n",
    "* Constructing a user preference vector from previously reviewed listings\n",
    "* Ranking unseen listings by similarity to the user profile\n",
    "\n",
    "\n",
    "**Listing encoder**\n",
    "\n",
    "Each listing is encoded into a dense vector using:\n",
    "* Structured features (neighbourhood, room type, property type, amenities, price, location)\n",
    "* Summary text embeddings to capture qualitative differences between properties\n",
    "* These are combined in a neural network to produce a compact listing embedding\n",
    "\n",
    "**User representation**  \n",
    "* A user is represented as a sentiment-weighted average of the embeddings of previously reviewed listings.\n",
    "* Positive reviews influence the profile more\n",
    "* Negative reviews reduce influence rather than acting as strict dislikes\n",
    "* This avoids overreacting to one-off negative experiences while still capturing overall preferences.\n",
    "\n",
    "**Training objective - Bayesian Personalized Ranking (BPR)**  \n",
    "* The model is trained using Bayesian Personalized Ranking (BPR), \n",
    "* This optimises reviewed listings so that they should rank higher than unobserved listings (no reviews)\n",
    "* This is appropriate for implicit feedback and directly aligns with the top-K recommendation objective\n",
    "\n",
    "**Hard negative sampling**  \n",
    "* First runs used uniform random sampling for negatives which made the ranking task too easy and didn't provide meaningful separation  \n",
    "* To improve discrimination between similar properties, negatives were sampled from listings sharing the same room type (this could further be extended to neighbourhood with more time)\n",
    "* This encouraged the model to distinguish between similar listings, resulting in improved ranking performance. (MRR)\n",
    "\n",
    "\n",
    "**Evaluation**\n",
    "* Performed using temporal hold-out\n",
    "* For each user, the last reviewed listing was held out\n",
    "* The model was trained on earlier reviews\n",
    "* Metrics reported: \n",
    "    * Recall@K: how often does the model retrieve the correct listing within a shortlist of size K\n",
    "    * MRR@K: how highly the correct listing is ranked when it appears in the top K.\n",
    "\n",
    "---\n",
    "\n",
    "## Baseline Comparison\n",
    "\n",
    "### Popularity Baseline\n",
    "\n",
    "* As a benchmark, a non-personalised popularity baseline was calculated\n",
    "* Listings were ranked by total number of reviews\n",
    "* For each user, previously reviewed listings were excluded from recommendations\n",
    "\n",
    "Results showed:\n",
    "\n",
    "* **Recall@10 ≈ 0.0**\n",
    "* **Recall@20 ≈ 0.0**\n",
    "* **Recall@1000 ≈ 16%**\n",
    "\n",
    "This indicates:\n",
    "\n",
    "* The catalogue is strongly long-tailed.\n",
    "* The held-out listing is rarely among the globally most popular items.\n",
    "* The task is non-trivial at small K.\n",
    "\n",
    "Popularity is then used as a lower bound for performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Experiment Results / Model Development\n",
    "\n",
    "Only around 12% of users have reviewed more than one property, so most users provide very limited behavioural history. This makes exact next-item prediction inherently difficult, particularly in a large and long-tailed catalogue.\n",
    "\n",
    "I therefore started with a simple structured-feature baseline and incrementally added complexity to evaluate which modelling changes produced meaningful improvements.\n",
    "\n",
    "### Structured Features Only\n",
    "\n",
    "Using only structured listing features:\n",
    "\n",
    "* **Recall@10 ≈ 1.1%**\n",
    "* **Recall@20 ≈ 1.4%**\n",
    "\n",
    "Observations:\n",
    "\n",
    "* The model slightly outperformed popularity at small K\n",
    "* However struggled to distinguish between structurally similar listings\n",
    "* This suggested that structured metadata alone was insufficient to fully capture user preference nuances\n",
    "\n",
    "---\n",
    "\n",
    "### Hard Negative Sampling\n",
    "\n",
    "Switching from uniform random negatives to same-room-type negatives:\n",
    "\n",
    "* Improved MRR\n",
    "* Slightly improved Recall@K stability\n",
    "* Increased training difficulty (loss decreased more slowly)\n",
    "\n",
    "Observations:\n",
    "\n",
    "* Uniform negatives made the ranking task too easy.  \n",
    "* Hard negatives forced the model to distinguish between similar listings, improving ranking performance even if recall improvements were modest\n",
    "\n",
    "---\n",
    "\n",
    "### Adding Summary Text Embeddings\n",
    "\n",
    "Incorporating semantic embeddings of the listing summary text:\n",
    "\n",
    "* Recall@10 improved from ~1.1% → ~1.6%\n",
    "* Recall@20 improved from ~1.4% → ~2.4%\n",
    "* MRR increased further\n",
    "* This represents a meaningful relative improvement\n",
    "\n",
    "Observations:\n",
    "\n",
    "* Structured features alone lack sufficient granularity\n",
    "* Summary text captures qualitative differences (style, ambiance, unique attributes)\n",
    "* Semantic representation significantly improves differentiation between similar properties\n",
    "\n",
    "This confirms that representation quality is very important to performance of the model\n",
    "\n",
    "---\n",
    "\n",
    "## Model Insights & Conclusions\n",
    "\n",
    "### Deeper Representation of Features is Required\n",
    "\n",
    "* Optimisation changes (more epochs or adjusting learning rate) yielded marginal improvements\n",
    "* Adding semantic text embeddings yielded significant improvements\n",
    "* Deeper representation of the text based features could significantly improve results given minimal experiments so far\n",
    "\n",
    "---\n",
    "\n",
    "### Personalisation is Limited by Data Sparsity\n",
    "\n",
    "* Only a small proportion of users have multiple reviews.\n",
    "* Most user profiles are built from very limited historical interactions\n",
    "* I evaluated the model using next item prediction, but with more time I would also look at general preference alignment, such as similarity to the held-out listing or neighbourhood level accuracy\n",
    "\n",
    "---\n",
    "\n",
    "### Sentiment Weighting as a Stabiliser\n",
    "\n",
    "* Sentiment was used as a soft weighting mechanism\n",
    "* Negative reviews reduced influence rather than acting as strict negative signals\n",
    "* This prevents overfitting to situational dissatisfaction\n",
    "* With more time I would look at modelling the key factors mentioned in the reviews and the positive/negative sentiment aligned to those\n",
    "\n",
    "---\n",
    "\n",
    "## Extensions & Future Improvements\n",
    "\n",
    "With additional time, the following extensions would likely improve performance:\n",
    "\n",
    "* **Aspect-level sentiment extraction**\n",
    "  * Extract sentiment on specific aspects (location, cleanliness, host, noise)\n",
    "  * Align aspect level sentiment directly with listing features\n",
    "  * This would provide more targeted preference signals rather than relying on overall review sentiment\n",
    "\n",
    "* **More advanced hard negative sampling**\n",
    "  * Sample within same neighbourhood and price band\n",
    "  * This could further improve the model's ability to distinguish similar listings\n",
    "\n",
    "* **Cold-Start Strategy for Single-Review Users**\n",
    "    * Given that only ~12% of users have multiple reviews, most users provide very limited history\n",
    "    * Nearest-neighbour retrieval\n",
    "        * Recommend listings most similar to the reviewed property using structured and semantic embeddings\n",
    "    * Adjusting recommendations when confidence is low\n",
    "        * Estimate confidence based on similarity strength and adjust ranking conservatively when signal is weak\n",
    "\n",
    "* **Online evaluation**  \n",
    "If deployed some of the evaluations used could focus on:  \n",
    "  * CTR\n",
    "  * Converstion rate / booking rate - do recommendations lead to bookings\n",
    "  * Diversity of recommnedations - we want varied recommnedations to ensure wide inventory is booked\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7485167",
   "metadata": {},
   "source": [
    "# Recommender Model Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b35f63",
   "metadata": {},
   "source": [
    "## Calculate sentiment of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99bb2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "MODEL_NAME = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "@torch.inference_mode()\n",
    "def sentiment_xlmr(\n",
    "    texts,\n",
    "    batch_size: int = 64,\n",
    "    max_length: int = 128,\n",
    "    device: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - label: {negative, neutral, positive}\n",
    "      - p_negative, p_neutral, p_positive\n",
    "      - sentiment_score: scalar in [-1, 1] computed as p_pos - p_neg\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Model label order is typically: negative, neutral, positive\n",
    "    id2label = model.config.id2label\n",
    "    # Ensure consistent column ordering\n",
    "    # We'll map probabilities by label name\n",
    "    out_labels = []\n",
    "    p_neg, p_neu, p_pos = [], [], []\n",
    "    score = []\n",
    "\n",
    "    # Clean texts\n",
    "    texts = [clean_text(t) for t in texts]\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Sentiment (XLM-R)\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        logits = model(**enc).logits\n",
    "        probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()  # shape (B, C)\n",
    "\n",
    "        for row in probs:\n",
    "            # Build dict {label_name: prob}\n",
    "            prob_by_label = {id2label[j].lower(): float(row[j]) for j in range(len(row))}\n",
    "            neg = prob_by_label.get(\"negative\", 0.0)\n",
    "            neu = prob_by_label.get(\"neutral\", 0.0)\n",
    "            pos = prob_by_label.get(\"positive\", 0.0)\n",
    "\n",
    "            # Predicted label\n",
    "            pred = max(prob_by_label, key=prob_by_label.get)\n",
    "\n",
    "            out_labels.append(pred)\n",
    "            p_neg.append(neg)\n",
    "            p_neu.append(neu)\n",
    "            p_pos.append(pos)\n",
    "\n",
    "            # Scalar sentiment in [-1, 1]\n",
    "            # (pos - neg) is a simple, interpretable signal for weighting interactions\n",
    "            score.append(pos - neg)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"sentiment_label\": out_labels,\n",
    "        \"p_negative\": p_neg,\n",
    "        \"p_neutral\": p_neu,\n",
    "        \"p_positive\": p_pos,\n",
    "        \"sentiment_score\": score,   # [-1, 1]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01758bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would normally batch the processing of this for production but for this task have run all together for simplicity\n",
    "\n",
    "run_sentiment_analysis = False\n",
    "\n",
    "if run_sentiment_analysis:\n",
    "    reviews = pd.read_csv('data/reviews.csv')\n",
    "    sent_df = sentiment_xlmr(reviews[\"comments\"].tolist(), batch_size=64, max_length=128)\n",
    "    reviews = pd.concat([reviews.reset_index(drop=True), sent_df], axis=1)\n",
    "    reviews.to_csv('data/reviews_sentiment.csv', index=False)\n",
    "else:\n",
    "    reviews = pd.read_csv('data/reviews_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcccc5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "      <th>word_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>lang_prob</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>p_negative</th>\n",
       "      <th>p_neutral</th>\n",
       "      <th>p_positive</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11551</td>\n",
       "      <td>30672</td>\n",
       "      <td>2010-03-21</td>\n",
       "      <td>93896</td>\n",
       "      <td>Shar-Lyn</td>\n",
       "      <td>The flat was bright, comfortable and clean and...</td>\n",
       "      <td>49</td>\n",
       "      <td>en</td>\n",
       "      <td>0.989361</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.041174</td>\n",
       "      <td>0.138245</td>\n",
       "      <td>0.820581</td>\n",
       "      <td>0.779408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11551</td>\n",
       "      <td>32236</td>\n",
       "      <td>2010-03-29</td>\n",
       "      <td>97890</td>\n",
       "      <td>Zane</td>\n",
       "      <td>We stayed with Adriano and Valerio for a week ...</td>\n",
       "      <td>46</td>\n",
       "      <td>en</td>\n",
       "      <td>0.965011</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.024325</td>\n",
       "      <td>0.100110</td>\n",
       "      <td>0.875564</td>\n",
       "      <td>0.851239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90700</td>\n",
       "      <td>337227</td>\n",
       "      <td>2011-06-27</td>\n",
       "      <td>311071</td>\n",
       "      <td>Miqua</td>\n",
       "      <td>it was all in all the perfect week!\\r\\nchilton...</td>\n",
       "      <td>84</td>\n",
       "      <td>en</td>\n",
       "      <td>0.986724</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.074755</td>\n",
       "      <td>0.112785</td>\n",
       "      <td>0.812459</td>\n",
       "      <td>0.737704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90700</td>\n",
       "      <td>378738</td>\n",
       "      <td>2011-07-17</td>\n",
       "      <td>224367</td>\n",
       "      <td>Prateek</td>\n",
       "      <td>I'll start with the host, and then move on to ...</td>\n",
       "      <td>189</td>\n",
       "      <td>en</td>\n",
       "      <td>0.993054</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.202945</td>\n",
       "      <td>0.295640</td>\n",
       "      <td>0.501414</td>\n",
       "      <td>0.298469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90700</td>\n",
       "      <td>543840</td>\n",
       "      <td>2011-09-18</td>\n",
       "      <td>1115024</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Great location. Plenty to do just steps outsid...</td>\n",
       "      <td>92</td>\n",
       "      <td>en</td>\n",
       "      <td>0.977307</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.046721</td>\n",
       "      <td>0.104764</td>\n",
       "      <td>0.848515</td>\n",
       "      <td>0.801794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485592</th>\n",
       "      <td>39740287</td>\n",
       "      <td>559509688</td>\n",
       "      <td>2019-11-04</td>\n",
       "      <td>182032644</td>\n",
       "      <td>Isabel</td>\n",
       "      <td>A very good stay, I would repeat for sure.</td>\n",
       "      <td>9</td>\n",
       "      <td>en</td>\n",
       "      <td>0.996551</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.020210</td>\n",
       "      <td>0.054513</td>\n",
       "      <td>0.925276</td>\n",
       "      <td>0.905066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485593</th>\n",
       "      <td>22701498</td>\n",
       "      <td>558667202</td>\n",
       "      <td>2019-11-03</td>\n",
       "      <td>65955902</td>\n",
       "      <td>Shereen</td>\n",
       "      <td>Set in a lovely development with onsite bar, c...</td>\n",
       "      <td>24</td>\n",
       "      <td>en</td>\n",
       "      <td>0.967389</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.010547</td>\n",
       "      <td>0.090022</td>\n",
       "      <td>0.899432</td>\n",
       "      <td>0.888885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485594</th>\n",
       "      <td>38398365</td>\n",
       "      <td>552239161</td>\n",
       "      <td>2019-10-21</td>\n",
       "      <td>60436496</td>\n",
       "      <td>Chee Ling</td>\n",
       "      <td>(Website hidden by Airbnb) a.best owner and ge...</td>\n",
       "      <td>31</td>\n",
       "      <td>en</td>\n",
       "      <td>0.891100</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.034510</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.852255</td>\n",
       "      <td>0.817745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485595</th>\n",
       "      <td>38398365</td>\n",
       "      <td>559541617</td>\n",
       "      <td>2019-11-04</td>\n",
       "      <td>97684167</td>\n",
       "      <td>Carolyn</td>\n",
       "      <td>This flat is perfection! Everything you need i...</td>\n",
       "      <td>54</td>\n",
       "      <td>en</td>\n",
       "      <td>0.975492</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.035403</td>\n",
       "      <td>0.072402</td>\n",
       "      <td>0.892195</td>\n",
       "      <td>0.856791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485596</th>\n",
       "      <td>39836317</td>\n",
       "      <td>559576331</td>\n",
       "      <td>2019-11-04</td>\n",
       "      <td>52497809</td>\n",
       "      <td>Aniello</td>\n",
       "      <td>You can expect to be staying is a very nice lo...</td>\n",
       "      <td>49</td>\n",
       "      <td>en</td>\n",
       "      <td>0.994694</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.070257</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.781228</td>\n",
       "      <td>0.710971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1485597 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         listing_id         id        date  reviewer_id reviewer_name  \\\n",
       "0             11551      30672  2010-03-21        93896      Shar-Lyn   \n",
       "1             11551      32236  2010-03-29        97890          Zane   \n",
       "2             90700     337227  2011-06-27       311071         Miqua   \n",
       "3             90700     378738  2011-07-17       224367       Prateek   \n",
       "4             90700     543840  2011-09-18      1115024      Jennifer   \n",
       "...             ...        ...         ...          ...           ...   \n",
       "1485592    39740287  559509688  2019-11-04    182032644        Isabel   \n",
       "1485593    22701498  558667202  2019-11-03     65955902       Shereen   \n",
       "1485594    38398365  552239161  2019-10-21     60436496     Chee Ling   \n",
       "1485595    38398365  559541617  2019-11-04     97684167       Carolyn   \n",
       "1485596    39836317  559576331  2019-11-04     52497809       Aniello   \n",
       "\n",
       "                                                  comments  word_count lang  \\\n",
       "0        The flat was bright, comfortable and clean and...          49   en   \n",
       "1        We stayed with Adriano and Valerio for a week ...          46   en   \n",
       "2        it was all in all the perfect week!\\r\\nchilton...          84   en   \n",
       "3        I'll start with the host, and then move on to ...         189   en   \n",
       "4        Great location. Plenty to do just steps outsid...          92   en   \n",
       "...                                                    ...         ...  ...   \n",
       "1485592         A very good stay, I would repeat for sure.           9   en   \n",
       "1485593  Set in a lovely development with onsite bar, c...          24   en   \n",
       "1485594  (Website hidden by Airbnb) a.best owner and ge...          31   en   \n",
       "1485595  This flat is perfection! Everything you need i...          54   en   \n",
       "1485596  You can expect to be staying is a very nice lo...          49   en   \n",
       "\n",
       "         lang_prob sentiment_label  p_negative  p_neutral  p_positive  \\\n",
       "0         0.989361        positive    0.041174   0.138245    0.820581   \n",
       "1         0.965011        positive    0.024325   0.100110    0.875564   \n",
       "2         0.986724        positive    0.074755   0.112785    0.812459   \n",
       "3         0.993054        positive    0.202945   0.295640    0.501414   \n",
       "4         0.977307        positive    0.046721   0.104764    0.848515   \n",
       "...            ...             ...         ...        ...         ...   \n",
       "1485592   0.996551        positive    0.020210   0.054513    0.925276   \n",
       "1485593   0.967389        positive    0.010547   0.090022    0.899432   \n",
       "1485594   0.891100        positive    0.034510   0.113235    0.852255   \n",
       "1485595   0.975492        positive    0.035403   0.072402    0.892195   \n",
       "1485596   0.994694        positive    0.070257   0.148515    0.781228   \n",
       "\n",
       "         sentiment_score  \n",
       "0               0.779408  \n",
       "1               0.851239  \n",
       "2               0.737704  \n",
       "3               0.298469  \n",
       "4               0.801794  \n",
       "...                  ...  \n",
       "1485592         0.905066  \n",
       "1485593         0.888885  \n",
       "1485594         0.817745  \n",
       "1485595         0.856791  \n",
       "1485596         0.710971  \n",
       "\n",
       "[1485597 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d004c",
   "metadata": {},
   "source": [
    "# Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfbff33",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5b1a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# -------------------------\n",
    "# Logging setup\n",
    "# -------------------------\n",
    "def setup_logger(name=\"recommender\", level=logging.INFO):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    if not logger.handlers:\n",
    "        h = logging.StreamHandler()\n",
    "        fmt = logging.Formatter(\n",
    "            fmt=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        )\n",
    "        h.setFormatter(fmt)\n",
    "        logger.addHandler(h)\n",
    "    logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "LOGGER = setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6c472",
   "metadata": {},
   "source": [
    "## Summary Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b351314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_embeddings(\n",
    "    listings: pd.DataFrame,\n",
    "    text_col: str = \"summary\",\n",
    "    model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    batch_size: int = 64,\n",
    "    logger: logging.Logger = LOGGER,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      emb: (n_listings, text_dim) float32 numpy array\n",
    "    \"\"\"\n",
    "    texts = listings[text_col].fillna(\"\").astype(str).tolist()\n",
    "    # light cleanup helps a bit\n",
    "    texts = [t.strip() if t.strip() else \"\" for t in texts]\n",
    "\n",
    "    logger.info(f\"Computing summary embeddings: n={len(texts)} model={model_name} batch_size={batch_size}\")\n",
    "    st = SentenceTransformer(model_name, device=\"cpu\")\n",
    "\n",
    "    emb = st.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    logger.info(f\"Summary embeddings ready: shape={emb.shape}\")\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a947d23",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63cbab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Preprocessing helpers\n",
    "# -------------------------\n",
    "def parse_money_to_float(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    if isinstance(x, (int, float, np.number)): return float(x)\n",
    "    s = str(x).strip().replace(\",\", \"\")\n",
    "    s = re.sub(r\"[^0-9\\.\\-]\", \"\", s)\n",
    "    try:\n",
    "        return float(s) if s else np.nan\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def normalize_amenities(amenities_str):\n",
    "    # clean text and return as list\n",
    "    if pd.isna(amenities_str): return []\n",
    "    s = str(amenities_str).strip().strip(\"{}\")\n",
    "    parts = [p.strip().strip('\"').strip(\"'\") for p in s.split(\",\")]\n",
    "    parts = [p.lower() for p in parts if p]\n",
    "    # light normalization\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        p = re.sub(r\"\\s+\", \"_\", p)\n",
    "        p = re.sub(r\"[^a-z0-9_\\-]+\", \"\", p)\n",
    "        if p:\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "def sentiment_to_weight(sentiment_score):\n",
    "    # convert sentiment_score from [-1,1] to [0,1] to use as interaction weight\n",
    "    s = np.clip(sentiment_score, -1, 1)\n",
    "    return (s + 1.0) / 2.0  # [0,1]\n",
    "\n",
    "# -------------------------\n",
    "# Build feature index / encodings\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class FeatureIndex:\n",
    "    neigh2i: Dict[str, int]\n",
    "    ptype2i: Dict[str, int]\n",
    "    rtype2i: Dict[str, int]\n",
    "    amen2i: Dict[str, int]\n",
    "    lid2i: Dict[int, int] \n",
    "    i2lid: List[int]\n",
    "\n",
    "def build_feature_index(listings: pd.DataFrame) -> FeatureIndex:\n",
    "    def make_map(vals):\n",
    "        vals = [v for v in vals if isinstance(v, str) and v.strip()]\n",
    "        uniq = sorted(set(vals))\n",
    "        return {v: i+1 for i, v in enumerate(uniq)}  # 0 UNK/none\n",
    "\n",
    "    neigh2i = make_map(listings[\"neighbourhood_cleansed\"].fillna(\"\").astype(str).tolist())\n",
    "    ptype2i = make_map(listings[\"property_type\"].fillna(\"\").astype(str).tolist())\n",
    "    rtype2i = make_map(listings[\"room_type\"].fillna(\"\").astype(str).tolist())\n",
    "\n",
    "    # amenities index\n",
    "    amen_counts = {}\n",
    "    for a_list in listings[\"amenities_list\"]:\n",
    "        for a in a_list:\n",
    "            amen_counts[a] = amen_counts.get(a, 0) + 1\n",
    "    # keep top amenities (selected 50 for now but would ideally tune this)\n",
    "    TOP_N_AMENITIES = 50\n",
    "    top_amen = sorted(amen_counts.items(), key=lambda x: x[1], reverse=True)[:TOP_N_AMENITIES]\n",
    "    amen2i = {a: i for i, (a, _) in enumerate(top_amen)}\n",
    "\n",
    "    # listing id map\n",
    "    i2lid = listings[\"id\"].astype(int).tolist()\n",
    "    lid2i = {lid: i for i, lid in enumerate(i2lid)}\n",
    "\n",
    "    return FeatureIndex(neigh2i, ptype2i, rtype2i, amen2i, lid2i, i2lid)\n",
    "\n",
    "def encode_listings(listings: pd.DataFrame, feature_index: FeatureIndex) -> Dict[str, torch.Tensor]:\n",
    "    # categorical features\n",
    "    neighbourhood_idx = listings[\"neighbourhood_cleansed\"].fillna(\"\").astype(str).map(lambda x: feature_index.neigh2i.get(x, 0)).to_numpy()\n",
    "    property_type_idx = listings[\"property_type\"].fillna(\"\").astype(str).map(lambda x: feature_index.ptype2i.get(x, 0)).to_numpy()\n",
    "    room_type_idx = listings[\"room_type\"].fillna(\"\").astype(str).map(lambda x: feature_index.rtype2i.get(x, 0)).to_numpy()\n",
    "\n",
    "    # numeric features\n",
    "    latitude = listings[\"latitude\"].astype(float).to_numpy()\n",
    "    longitude = listings[\"longitude\"].astype(float).to_numpy()\n",
    "    accommodates = listings[\"accommodates\"].astype(float).fillna(0).to_numpy()\n",
    "\n",
    "    total_price = listings[\"total_price\"].astype(float).to_numpy()\n",
    "    valid_mask = np.isfinite(total_price) & (total_price > 0)\n",
    "    total_price = np.where(valid_mask, np.log1p(total_price), 0.0)\n",
    "\n",
    "    numeric_features = np.stack([latitude, longitude, accommodates, total_price], axis=1).astype(\"float32\")\n",
    "\n",
    "    # normalize numeric\n",
    "    num_mean = np.nanmean(numeric_features, axis=0)\n",
    "    num_std  = np.nanstd(numeric_features, axis=0) + 1e-6\n",
    "    numeric_features = np.nan_to_num((numeric_features - num_mean) / num_std, nan=0.0)\n",
    "\n",
    "    # amenities multi-hot\n",
    "    n_listings = len(listings)\n",
    "    n_amen = len(feature_index.amen2i)\n",
    "\n",
    "    amenity_matrix = np.zeros((n_listings, n_amen), dtype=np.float32)\n",
    "\n",
    "    for i, amenities in enumerate(listings[\"amenities_list\"]):\n",
    "        for a in amenities:\n",
    "            idx = feature_index.amen2i.get(a)\n",
    "            if idx is not None:\n",
    "                amenity_matrix[i, idx] = 1.0\n",
    "\n",
    "    return {\n",
    "        \"neighbourhood_idx\": torch.tensor(neighbourhood_idx, dtype=torch.long),\n",
    "        \"property_type_idx\": torch.tensor(property_type_idx, dtype=torch.long),\n",
    "        \"room_type_idx\": torch.tensor(room_type_idx, dtype=torch.long),\n",
    "        \"numeric_features\": torch.tensor(numeric_features, dtype=torch.float32),\n",
    "        \"amenity_features\": torch.tensor(amenity_matrix, dtype=torch.float32),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d0b651",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "365ce949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Listing encoder model\n",
    "# -------------------------\n",
    "class ListingEncoder(nn.Module):\n",
    "    def __init__(self, n_neigh, n_ptype, n_rtype, n_amen, d_numeric, text_dim, d=64, out_dim=64):\n",
    "        super().__init__()\n",
    "        # categorical embeddings\n",
    "        self.neighbourhood_embedding = nn.Embedding(n_neigh, d)\n",
    "        self.property_type_embedding = nn.Embedding(n_ptype, d)\n",
    "        self.room_type_embedding = nn.Embedding(n_rtype, d)\n",
    "\n",
    "        # numeric projection\n",
    "        self.numeric_projection = nn.Sequential(\n",
    "            nn.Linear(d_numeric, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, d),\n",
    "        )\n",
    "\n",
    "        # amentities projection\n",
    "        self.amenity_linear = nn.Linear(n_amen, d)\n",
    "\n",
    "        # summary text projection\n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(text_dim, d),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # final mlp\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(d*6, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, neighbourhood_idx, property_type_idx, room_type_idx, numeric_features, amenity_features, summary_embedding):\n",
    "        e_neigh = self.neighbourhood_embedding(neighbourhood_idx)\n",
    "        e_ptype = self.property_type_embedding(property_type_idx)\n",
    "        e_rtype = self.room_type_embedding(room_type_idx)\n",
    "\n",
    "        e_num = self.numeric_projection(numeric_features)\n",
    "        e_amen = self.amenity_linear(amenity_features)\n",
    "        e_txt = self.text_proj(summary_embedding)\n",
    "\n",
    "        combined_features = torch.cat([e_neigh, e_ptype, e_rtype, e_amen, e_num, e_txt], dim=1)\n",
    "\n",
    "        listing_embedding = self.final_mlp(combined_features)\n",
    "        listing_embedding = F.normalize(listing_embedding, dim=1)  \n",
    "        return listing_embedding\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Training dataset: next-item pairs\n",
    "# -------------------------\n",
    "class NextItemDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Builds training samples for next-item prediction from time-ordered reviews per user.\n",
    "    Each sample is (history_listing_indices, history_weights, pos_listing_index)\n",
    "    where weights come from sentiment_to_weight(sentiment_score).\n",
    "    Note:\n",
    "      - This dataset only yields samples for users with >= 2 reviews.\n",
    "      - Single-review users are still recommendable at inference time, but they do not\n",
    "        contribute to next-item training/evaluation under this setup.\n",
    "    \"\"\"\n",
    "    def __init__(self, reviews: pd.DataFrame, feature_index: FeatureIndex, min_hist=1):\n",
    "        df = reviews.loc[:, [\"reviewer_id\", \"listing_id\", \"date\", \"sentiment_score\"]].copy()\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"reviewer_id\", \"listing_id\", \"date\"])\n",
    "\n",
    "        df[\"listing_id\"] = df[\"listing_id\"].astype(int)\n",
    "        df[\"reviewer_id\"] = df[\"reviewer_id\"].astype(str)\n",
    "\n",
    "        df = df[df[\"listing_id\"].isin(feature_index.lid2i)]\n",
    "        df[\"listing_idx\"] = df[\"listing_id\"].map(feature_index.lid2i).astype(int)\n",
    "        df[\"weight\"] = df[\"sentiment_score\"].astype(float).map(sentiment_to_weight)\n",
    "\n",
    "        df = df.sort_values([\"reviewer_id\", \"date\"])\n",
    "\n",
    "        samples: list[tuple[list[int], list[float], int]] = []\n",
    "\n",
    "        for _, g in df.groupby(\"reviewer_id\", sort=False):\n",
    "            seq = g[\"listing_idx\"].to_list()\n",
    "            wts = g[\"weight\"].to_list()\n",
    "\n",
    "            # Need at least 2 interactions to form (history -> next item)\n",
    "            if len(seq) < 2:\n",
    "                continue\n",
    "\n",
    "            # Create next-item samples\n",
    "            for t in range(1, len(seq)):\n",
    "                hist = seq[:t]\n",
    "                hist_w = wts[:t]\n",
    "                pos = seq[t]\n",
    "                if len(hist) >= min_hist:\n",
    "                    samples.append((hist, hist_w, pos))\n",
    "\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.samples[idx]\n",
    "    \n",
    "\n",
    "def collate_batch(batch, max_hist=50):\n",
    "    # pad histories to a fixed length\n",
    "    hists, weights, pos = zip(*batch)\n",
    "\n",
    "    B = len(hists)\n",
    "    L = min(max_hist, max(len(h) for h in hists))\n",
    "\n",
    "    hist_pad = torch.zeros((B, L), dtype=torch.long)\n",
    "    w_pad = torch.zeros((B, L), dtype=torch.float32)\n",
    "    mask = torch.zeros((B, L), dtype=torch.float32)\n",
    "\n",
    "    for i, (h, w) in enumerate(zip(hists, weights)):\n",
    "        h = h[-L:]  # keep most recent L items\n",
    "        w = w[-L:]\n",
    "\n",
    "        n = len(h)\n",
    "        hist_pad[i, :n] = torch.tensor(h, dtype=torch.long)\n",
    "        w_pad[i, :n] = torch.tensor(w, dtype=torch.float32)\n",
    "        mask[i, :n] = 1.0\n",
    "\n",
    "    pos = torch.tensor(pos, dtype=torch.long)\n",
    "    return hist_pad, w_pad, mask, pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda8958",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5694c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_room_type_buckets(listing_feats):\n",
    "    \"\"\"\n",
    "    Returns: dict {room_type_id: tensor_of_listing_indices}\n",
    "    \"\"\"\n",
    "    room_type_idx = listing_feats[\"room_type_idx\"]\n",
    "    buckets = {}\n",
    "\n",
    "    for idx, rt in enumerate(room_type_idx.tolist()):\n",
    "        buckets.setdefault(rt, []).append(idx)\n",
    "\n",
    "    # convert lists to tensors for fast sampling\n",
    "    for rt in buckets:\n",
    "        buckets[rt] = torch.tensor(buckets[rt], dtype=torch.long)\n",
    "\n",
    "    return buckets\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    listing_feats: dict,\n",
    "    dataset,\n",
    "    n_listings: int,\n",
    "    collate_batch,\n",
    "    device: str | None = None,\n",
    "    epochs: int = 2,\n",
    "    batch_size: int = 256,\n",
    "    lr: float = 2e-3,\n",
    "    neg_k: int = 10,\n",
    "    logger: logging.Logger = LOGGER,\n",
    "):\n",
    "\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    logger.info(\"==== Training start ====\")\n",
    "    logger.info(f\"device={device} epochs={epochs} batch_size={batch_size} lr={lr} neg_k={neg_k}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_batch,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # move listing features to device\n",
    "    feats = {k: v.to(device) for k, v in listing_feats.items()}\n",
    "    n_batches = len(loader)\n",
    "    logger.info(f\"dataset_samples={len(dataset)} batches_per_epoch={n_batches}\")\n",
    "\n",
    "    # room type buckets for negative sampling\n",
    "    room_type_buckets = build_room_type_buckets(listing_feats)\n",
    "\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        n_seen = 0\n",
    "\n",
    "        for hist, w, mask, pos in loader:\n",
    "            hist = hist.to(device)\n",
    "            w = w.to(device)\n",
    "            mask = mask.to(device)\n",
    "            pos = pos.to(device)\n",
    "\n",
    "            B, L = hist.shape\n",
    "\n",
    "            # Encode history listings\n",
    "            hist_neigh = feats[\"neighbourhood_idx\"][hist]\n",
    "            hist_ptype = feats[\"property_type_idx\"][hist]\n",
    "            hist_rtype = feats[\"room_type_idx\"][hist]\n",
    "            hist_num   = feats[\"numeric_features\"][hist]\n",
    "            hist_amen  = feats[\"amenity_features\"][hist]\n",
    "            hist_txt   = feats[\"summary_embedding\"][hist]\n",
    "\n",
    "            v_hist = model(\n",
    "                hist_neigh.reshape(-1),\n",
    "                hist_ptype.reshape(-1),\n",
    "                hist_rtype.reshape(-1),\n",
    "                hist_num.reshape(-1, hist_num.shape[-1]),\n",
    "                hist_amen.reshape(-1, hist_amen.shape[-1]),\n",
    "                hist_txt.reshape(-1, hist_txt.shape[-1]),\n",
    "            ).reshape(B, L, -1)\n",
    "\n",
    "            # User embedding (weighted average)\n",
    "            w_eff = w * mask\n",
    "            denom = w_eff.sum(dim=1, keepdim=True).clamp_min(1e-6)\n",
    "            u = (v_hist * w_eff.unsqueeze(-1)).sum(dim=1) / denom\n",
    "            u = F.normalize(u, dim=1)\n",
    "\n",
    "            # Positive item\n",
    "            v_pos = model(\n",
    "                feats[\"neighbourhood_idx\"][pos],\n",
    "                feats[\"property_type_idx\"][pos],\n",
    "                feats[\"room_type_idx\"][pos],\n",
    "                feats[\"numeric_features\"][pos],\n",
    "                feats[\"amenity_features\"][pos],\n",
    "                feats[\"summary_embedding\"][pos],\n",
    "            )\n",
    "\n",
    "            # Negative sampling\n",
    "            #neg = torch.randint(0, n_listings, (B, neg_k), device=device)\n",
    "            # Hard negatives: same room_type as positive\n",
    "            pos_room_types = listing_feats[\"room_type_idx\"][pos].to(device)\n",
    "\n",
    "            neg_list = []\n",
    "\n",
    "            for i in range(B):\n",
    "                rt = int(pos_room_types[i].item())\n",
    "                candidates = room_type_buckets.get(rt)\n",
    "\n",
    "                if candidates is None or len(candidates) == 0:\n",
    "                    # fallback to random if no candidates\n",
    "                    sampled = torch.randint(0, n_listings, (neg_k,), device=device)\n",
    "                else:\n",
    "                    # sample with replacement\n",
    "                    rand_idx = torch.randint(0, len(candidates), (neg_k,))\n",
    "                    sampled = candidates[rand_idx].to(device)\n",
    "\n",
    "                neg_list.append(sampled)\n",
    "\n",
    "            neg = torch.stack(neg_list, dim=0)  # shape (B, neg_k)\n",
    "\n",
    "            v_neg = model(\n",
    "                feats[\"neighbourhood_idx\"][neg].reshape(-1),\n",
    "                feats[\"property_type_idx\"][neg].reshape(-1),\n",
    "                feats[\"room_type_idx\"][neg].reshape(-1),\n",
    "                feats[\"numeric_features\"][neg].reshape(-1, feats[\"numeric_features\"].shape[-1]),\n",
    "                feats[\"amenity_features\"][neg].reshape(-1, feats[\"amenity_features\"].shape[-1]),\n",
    "                feats[\"summary_embedding\"][neg].reshape(-1, feats[\"summary_embedding\"].shape[-1]),\n",
    "            ).reshape(B, neg_k, -1)\n",
    "\n",
    "            # BPR (Bayesian Personalized Ranking) loss\n",
    "            s_pos = (u * v_pos).sum(dim=1, keepdim=True)\n",
    "            s_neg = (u.unsqueeze(1) * v_neg).sum(dim=2)\n",
    "\n",
    "            loss = -F.logsigmoid(s_pos - s_neg).mean()\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * B\n",
    "            n_seen += B\n",
    "\n",
    "        avg_loss = epoch_loss / max(n_seen, 1)\n",
    "        logger.info(f\"Epoch {epoch}/{epochs} - loss: {avg_loss:.4f}\")\n",
    "\n",
    "    logger.info(\"=== Training finished ===\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280be2ef",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f150a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Precompute listing embeddings\n",
    "# -------------------------\n",
    "@torch.inference_mode()\n",
    "def compute_all_listing_embeddings(\n",
    "    model: nn.Module,\n",
    "    listing_feats: dict,\n",
    "    batch_size: int = 4096,\n",
    "    device: str | None = None,\n",
    "    logger: logging.Logger = LOGGER,\n",
    "):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    feats = {k: v.to(device) for k, v in listing_feats.items()}\n",
    "\n",
    "    n_listings = feats[\"neighbourhood_idx\"].shape[0]\n",
    "    logger.info(f\"Precomputing listing embeddings (n={n_listings}, batch_size={batch_size}, device={device})\")\n",
    "\n",
    "    embeddings = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for start in range(0, n_listings, batch_size):\n",
    "        end = min(start + batch_size, n_listings)\n",
    "\n",
    "        batch_embeddings = model(\n",
    "            feats[\"neighbourhood_idx\"][start:end],\n",
    "            feats[\"property_type_idx\"][start:end],\n",
    "            feats[\"room_type_idx\"][start:end],\n",
    "            feats[\"numeric_features\"][start:end],\n",
    "            feats[\"amenity_features\"][start:end],\n",
    "            feats[\"summary_embedding\"][start:end],\n",
    "        )\n",
    "\n",
    "        embeddings.append(batch_embeddings.cpu())\n",
    "\n",
    "        logger.info(f\"  processed {end}/{n_listings}\")\n",
    "\n",
    "    listing_embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"Finished embedding precompute. Shape={tuple(listing_embeddings.shape)} time={elapsed:.1f}s\")\n",
    "\n",
    "    return listing_embeddings\n",
    "\n",
    "\n",
    "# listings preprocessing\n",
    "def prepare_listings(listings: pd.DataFrame) -> pd.DataFrame:\n",
    "    listings = listings.copy()\n",
    "    listings[\"id\"] = listings[\"id\"].astype(int)\n",
    "\n",
    "    listings[\"total_price\"] = listings[\"price\"].map(parse_money_to_float)\n",
    "    listings[\"amenities_list\"] = listings[\"amenities\"].map(normalize_amenities)\n",
    "\n",
    "    listings = listings.dropna(subset=[\"latitude\", \"longitude\", \"accommodates\", \"total_price\"])\n",
    "    listings[\"accommodates\"] = listings[\"accommodates\"].astype(float)\n",
    "    listings[\"total_price\"] = listings[\"total_price\"].astype(float)\n",
    "    return listings\n",
    "\n",
    "# -------------------------\n",
    "# Run training\n",
    "# -------------------------\n",
    "\n",
    "def run_training(listings: pd.DataFrame, reviews: pd.DataFrame):\n",
    "    listings = prepare_listings(listings)\n",
    "\n",
    "    summary_emb = compute_summary_embeddings(listings, text_col=\"summary\", batch_size=64)    \n",
    "    \n",
    "    feature_index = build_feature_index(listings)\n",
    "    listing_feats = encode_listings(listings, feature_index)\n",
    "    listing_feats[\"summary_embedding\"] = torch.tensor(summary_emb, dtype=torch.float32)\n",
    "    text_dim = listing_feats[\"summary_embedding\"].shape[1]\n",
    "\n",
    "    dataset = NextItemDataset(reviews, feature_index, min_hist=1)\n",
    "\n",
    "    model = ListingEncoder(\n",
    "        n_neigh=max(feature_index.neigh2i.values(), default=0) + 1,\n",
    "        n_ptype=max(feature_index.ptype2i.values(), default=0) + 1,\n",
    "        n_rtype=max(feature_index.rtype2i.values(), default=0) + 1,\n",
    "        n_amen=max(feature_index.amen2i.values(), default=0) + 1,\n",
    "        d_numeric=4,\n",
    "        text_dim=text_dim,\n",
    "        d=64,\n",
    "        out_dim=64,\n",
    "    )\n",
    "\n",
    "    model = train(\n",
    "        model=model,\n",
    "        listing_feats=listing_feats,\n",
    "        dataset=dataset,\n",
    "        n_listings=len(feature_index.i2lid),\n",
    "        collate_batch=collate_batch,\n",
    "        epochs=20,\n",
    "        batch_size=256,\n",
    "        lr=0.002,\n",
    "        neg_k=20,\n",
    "    )\n",
    "\n",
    "    # Precompute listing embeddings\n",
    "    listing_embeddings = compute_all_listing_embeddings(model, listing_feats, batch_size=4096)\n",
    "\n",
    "    return model, feature_index, listing_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cdda74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qn/f2jdz13n30n9d4ztmqh1htp80000gr/T/ipykernel_89341/4002194498.py:6: DtypeWarning: Columns (0: weekly_price, 1: monthly_price, 2: license, 3: jurisdiction_names) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  listings = pd.read_csv(LISTINGS_PATH)\n",
      "2026-02-26 19:17:35 | INFO | recommender | Computing summary embeddings: n=85068 model=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 batch_size=64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85408300d2674b4d98d8c10ac8c17c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba66b170c534ad08186ff475c5e3a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 19:22:42 | INFO | recommender | Summary embeddings ready: shape=(85068, 384)\n",
      "2026-02-26 19:23:21 | INFO | recommender | ==== Training start ====\n",
      "2026-02-26 19:23:21 | INFO | recommender | device=cpu epochs=20 batch_size=256 lr=0.002 neg_k=20\n",
      "2026-02-26 19:23:21 | INFO | recommender | dataset_samples=253115 batches_per_epoch=989\n",
      "2026-02-26 19:24:00 | INFO | recommender | Epoch 1/20 - loss: 0.4805\n",
      "2026-02-26 19:24:39 | INFO | recommender | Epoch 2/20 - loss: 0.4655\n",
      "2026-02-26 19:25:18 | INFO | recommender | Epoch 3/20 - loss: 0.4584\n",
      "2026-02-26 19:25:57 | INFO | recommender | Epoch 4/20 - loss: 0.4530\n",
      "2026-02-26 19:26:35 | INFO | recommender | Epoch 5/20 - loss: 0.4486\n",
      "2026-02-26 19:27:15 | INFO | recommender | Epoch 6/20 - loss: 0.4452\n",
      "2026-02-26 19:27:52 | INFO | recommender | Epoch 7/20 - loss: 0.4418\n",
      "2026-02-26 19:28:30 | INFO | recommender | Epoch 8/20 - loss: 0.4390\n",
      "2026-02-26 19:29:08 | INFO | recommender | Epoch 9/20 - loss: 0.4365\n",
      "2026-02-26 19:29:46 | INFO | recommender | Epoch 10/20 - loss: 0.4342\n",
      "2026-02-26 19:30:25 | INFO | recommender | Epoch 11/20 - loss: 0.4322\n",
      "2026-02-26 19:31:03 | INFO | recommender | Epoch 12/20 - loss: 0.4305\n",
      "2026-02-26 19:31:41 | INFO | recommender | Epoch 13/20 - loss: 0.4288\n",
      "2026-02-26 19:32:19 | INFO | recommender | Epoch 14/20 - loss: 0.4272\n",
      "2026-02-26 19:32:57 | INFO | recommender | Epoch 15/20 - loss: 0.4261\n",
      "2026-02-26 19:33:36 | INFO | recommender | Epoch 16/20 - loss: 0.4248\n",
      "2026-02-26 19:34:16 | INFO | recommender | Epoch 17/20 - loss: 0.4239\n",
      "2026-02-26 19:34:55 | INFO | recommender | Epoch 18/20 - loss: 0.4228\n",
      "2026-02-26 19:35:32 | INFO | recommender | Epoch 19/20 - loss: 0.4216\n",
      "2026-02-26 19:36:10 | INFO | recommender | Epoch 20/20 - loss: 0.4210\n",
      "2026-02-26 19:36:10 | INFO | recommender | === Training finished ===\n",
      "2026-02-26 19:36:10 | INFO | recommender | Precomputing listing embeddings (n=85068, batch_size=4096, device=cpu)\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 4096/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 8192/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 12288/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 16384/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 20480/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 24576/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 28672/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 32768/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 36864/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 40960/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 45056/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 49152/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 53248/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 57344/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 61440/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 65536/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 69632/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 73728/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 77824/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 81920/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender |   processed 85068/85068\n",
      "2026-02-26 19:36:10 | INFO | recommender | Finished embedding precompute. Shape=(85068, 64) time=0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model. Listing embedding matrix shape: (85068, 64)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "LISTINGS_PATH = \"data/listings.csv\"   \n",
    "REVIEWS_PATH  = \"data/reviews_sentiment.csv\"\n",
    "\n",
    "listings = pd.read_csv(LISTINGS_PATH)\n",
    "reviews  = pd.read_csv(REVIEWS_PATH)\n",
    "\n",
    "reviews[\"date\"] = pd.to_datetime(reviews[\"date\"], errors=\"coerce\")\n",
    "reviews = reviews.dropna(subset=[\"date\", \"reviewer_id\", \"listing_id\"])\n",
    "reviews[\"reviewer_id\"] = reviews[\"reviewer_id\"].astype(str)\n",
    "reviews[\"listing_id\"] = reviews[\"listing_id\"].astype(int)\n",
    "\n",
    "listings[\"id\"] = listings[\"id\"].astype(int)\n",
    "\n",
    "# run model\n",
    "\n",
    "model, feature_index, listing_embeddings = run_training(listings, reviews)\n",
    "\n",
    "print(\"Trained model. Listing embedding matrix shape:\", tuple(listing_embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e5563",
   "metadata": {},
   "source": [
    "## Get user recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2176f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "USER 42827397 | reviews=2 | unique_listings=2\n",
      "Last 3 reviewed listings: [14518189, 17658798]\n",
      "Top recommendations: [8203315, 13026922, 7495122, 22480412, 7313478, 7164478, 12899998, 3957718, 3994973, 6506872]\n",
      "\n",
      "Reviewed listing details:\n",
      "                                               name neighbourhood_cleansed     room_type property_type  accommodates   price\n",
      "id                                                                                                                          \n",
      "14518189  Cellar room with Shared Bathroom & Toilet            Westminster  Private room     Apartment             1  $19.00\n",
      "17658798              Double room in Balham, London             Wandsworth  Private room     Apartment             1  $50.00\n",
      "\n",
      "Recommended listing details:\n",
      "                                                       name neighbourhood_cleansed        room_type      property_type  accommodates   price\n",
      "id                                                                                                                                          \n",
      "8203315   Battersea Park + garden, piano, cat & super-host.             Wandsworth     Private room          Townhouse             1  $40.00\n",
      "13026922                    Traditional double room in home             Wandsworth     Private room  Bed and breakfast             1  $35.00\n",
      "7495122      Perfect place - CLAPHAM JUNCTION! With parking             Wandsworth  Entire home/apt          Apartment             4  $71.00\n",
      "22480412                      Brixton O2 Penthouse En-Suite                Lambeth     Private room          Apartment             2  $68.00\n",
      "7313478                          Lovely, large, bright room             Wandsworth     Private room              House             1  $25.00\n",
      "7164478                      Lovely London Double bedroom**             Wandsworth     Private room          Apartment             2  $33.00\n",
      "12899998        1 bedroom flat in Clapham (Common/Junction)             Wandsworth  Entire home/apt          Apartment             4  $99.00\n",
      "3957718                                         Cosy London             Wandsworth     Private room          Apartment             1  $31.00\n",
      "3994973                        London Clapham Apartment - 8                Lambeth  Entire home/apt          Apartment             4  $66.00\n",
      "6506872                     Double room with ensuite shower             Wandsworth     Private room          Townhouse             2  $50.00\n",
      "\n",
      "================================================================================\n",
      "USER 121670412 | reviews=13 | unique_listings=9\n",
      "Last 3 reviewed listings: [29323898, 24325920, 32517730]\n",
      "Top recommendations: [35840170, 17690966, 21116605, 38230224, 17083815, 20733838, 16158534, 22318763, 20963715, 25478330]\n",
      "\n",
      "Reviewed listing details:\n",
      "                                                        name neighbourhood_cleansed        room_type property_type  accommodates    price\n",
      "id                                                                                                                                       \n",
      "27954279                               Modern Docklands flat          Tower Hamlets  Entire home/apt     Apartment             2  $100.00\n",
      "17976194   Modern & stylish 2 bed in Canary Wharf w/ balcony          Tower Hamlets  Entire home/apt     Apartment             6   $99.00\n",
      "27973895     Charming and Spacious Apartment in Canning Town                 Newham  Entire home/apt     Apartment             2   $43.00\n",
      "12119228                   1 bedroom flat ideal for 4 people          Tower Hamlets  Entire home/apt     Apartment             4  $105.00\n",
      "27973895     Charming and Spacious Apartment in Canning Town                 Newham  Entire home/apt     Apartment             2   $43.00\n",
      "27973895     Charming and Spacious Apartment in Canning Town                 Newham  Entire home/apt     Apartment             2   $43.00\n",
      "16670064  Homes for good 2bed2bath  - 4 min from underground          Tower Hamlets  Entire home/apt     Apartment             8  $110.00\n",
      "29855116         1 Bedroom Flat with Stunning Views of Canal          Tower Hamlets  Entire home/apt     Apartment             4  $100.00\n",
      "27973895     Charming and Spacious Apartment in Canning Town                 Newham  Entire home/apt     Apartment             2   $43.00\n",
      "27973895     Charming and Spacious Apartment in Canning Town                 Newham  Entire home/apt     Apartment             2   $43.00\n",
      "29323898   LuxuryApartment 5mins walk CanningTown JubileeDLR          Tower Hamlets  Entire home/apt     Apartment             4  $135.00\n",
      "24325920  Beautiful 1Bed w/Private Balcony by Regent’s Canal          Tower Hamlets  Entire home/apt     Apartment             4   $65.00\n",
      "32517730    Modern Studio Flat in Canary Wharf with balcony.          Tower Hamlets  Entire home/apt     Apartment             3   $80.00\n",
      "\n",
      "Recommended listing details:\n",
      "                                                        name neighbourhood_cleansed        room_type property_type  accommodates   price\n",
      "id                                                                                                                                      \n",
      "35840170                 2 bedroom flat in east India/London          Tower Hamlets  Entire home/apt     Apartment             3  $80.00\n",
      "17690966                Single room in Poplar (Canary Wharf)          Tower Hamlets     Private room     Apartment             1  $15.00\n",
      "21116605              Entire Apartment Canary Wharf 4 Guests          Tower Hamlets  Entire home/apt     Apartment             4  $70.00\n",
      "38230224     One bedroom penthouse - Excel, O2, Canary Wharf                 Newham  Entire home/apt   Condominium             3  $77.00\n",
      "17083815                                      Locksons Wharf          Tower Hamlets  Entire home/apt     Apartment             4  $97.00\n",
      "20733838     Newly built apartment in Bow, near Canary Wharf          Tower Hamlets  Entire home/apt     Apartment             3  $85.00\n",
      "16158534  Home with a hammock; enjoy the sun on the balcony!          Tower Hamlets  Entire home/apt     Apartment             4  $72.00\n",
      "22318763      This is a Lovely single room in Canary Wharf p          Tower Hamlets     Private room         House             1  $30.00\n",
      "20963715              Entire Apartment Canary Wharf 4 Guests          Tower Hamlets  Entire home/apt     Apartment             4  $90.00\n",
      "25478330              Stylish 2 Bed Apartment 1 min from DLR          Tower Hamlets  Entire home/apt     Apartment             4  $92.00\n",
      "\n",
      "================================================================================\n",
      "USER 57883008 | reviews=3 | unique_listings=2\n",
      "Last 3 reviewed listings: [2386245, 2386245, 8279354]\n",
      "Top recommendations: [13275704, 3123465, 8452672, 8041285, 5869526, 12803453, 39384571, 2213040, 5290104, 8393124]\n",
      "\n",
      "Reviewed listing details:\n",
      "                                                name neighbourhood_cleansed     room_type property_type  accommodates   price\n",
      "id                                                                                                                           \n",
      "2386245  Charming room in bright, pretty garden flat   Richmond upon Thames  Private room     Apartment             1  $34.00\n",
      "2386245  Charming room in bright, pretty garden flat   Richmond upon Thames  Private room     Apartment             1  $34.00\n",
      "8279354                         Large single bedroom   Richmond upon Thames  Private room         House             1  $30.00\n",
      "\n",
      "Recommended listing details:\n",
      "                                                    name neighbourhood_cleansed        room_type property_type  accommodates   price\n",
      "id                                                                                                                                  \n",
      "13275704   Airy double room in family home in Teddington   Richmond upon Thames     Private room         House             2  $50.00\n",
      "3123465              Lovely large Room in Twickenham TW2   Richmond upon Thames     Private room         House             1  $30.00\n",
      "8452672   Lovely Loft Room + Ensuite + TV + Free Parking   Richmond upon Thames     Private room          Loft             2  $47.00\n",
      "8041285                Peaceful double in great location   Richmond upon Thames     Private room         House             2  $40.00\n",
      "5869526               Beautiful studio loft room ensuite   Richmond upon Thames     Private room         House             4  $48.00\n",
      "12803453   London studio flat with parking near Heathrow               Hounslow  Entire home/apt     Apartment             2  $75.00\n",
      "39384571                          Comfy, cosy and homely   Richmond upon Thames     Private room     Apartment             2  $35.00\n",
      "2213040              Ground floor ensuite double bedroom   Richmond upon Thames     Private room         House             2  $25.00\n",
      "5290104               Double bedroom - quiet modern flat   Richmond upon Thames     Private room     Apartment             1  $28.00\n",
      "8393124              One or Two Rooms in Twickenham Home   Richmond upon Thames     Private room         House             3  $60.00\n"
     ]
    }
   ],
   "source": [
    "def recommend_for_user(user_reviews: pd.DataFrame, feature_index: FeatureIndex, listing_embeddings: torch.Tensor, k: int = 20) -> list[int]:\n",
    "\n",
    "    if user_reviews.empty: return []\n",
    "\n",
    "    user_reviews = user_reviews.sort_values(\"date\")\n",
    "    # history listing indices and weights\n",
    "    listing_indices = [feature_index.lid2i.get(int(lid)) for lid in user_reviews[\"listing_id\"].astype(int).tolist()]\n",
    "    weights = [sentiment_to_weight(float(s)) for s in user_reviews[\"sentiment_score\"].astype(float).tolist()]\n",
    "\n",
    "    # keep only ineractions that are in the index\n",
    "    history = [(idx, w) for idx, w in zip(listing_indices, weights) if idx is not None]\n",
    "    if not history: return []\n",
    "\n",
    "    seen_indices = {idx for idx, _ in history}\n",
    "    hist_idx = torch.tensor([idx for idx, _ in history], dtype=torch.long)\n",
    "    hist_wts = torch.tensor([w for _, w in history], dtype=torch.float32)\n",
    "\n",
    "    # user embedding: weighted average of listing embeddings\n",
    "    user_vec = (listing_embeddings[hist_idx] * hist_wts.unsqueeze(1)).sum(dim=0)\n",
    "    user_vec = user_vec / (hist_wts.sum() + 1e-9)\n",
    "    user_vec = F.normalize(user_vec, dim=0)\n",
    "\n",
    "    # Compute scores via dot product (cosine similarity because embeddings are normalized)\n",
    "    scores = torch.mv(listing_embeddings, user_vec).cpu().numpy()\n",
    "    # exclude seen\n",
    "    scores[list(seen_indices)] = -1e9\n",
    "\n",
    "    # Sort all listings by score descending\n",
    "    sorted_idx = np.argsort(-scores)\n",
    "\n",
    "    # Take top-k\n",
    "    top_k_idx = sorted_idx[:k]\n",
    "\n",
    "    return [feature_index.i2lid[i] for i in top_k_idx]\n",
    "\n",
    "def pick_returning_users(reviews_df, n=5, min_reviews=3, random_state=42):\n",
    "    counts = reviews_df.groupby(\"reviewer_id\")[\"listing_id\"].nunique()\n",
    "    eligible = counts[counts >= min_reviews].index.tolist()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    picks = rng.choice(eligible, size=min(n, len(eligible)), replace=False)\n",
    "    return [str(x) for x in picks]\n",
    "\n",
    "def show_user_recs(user_id, k=10):\n",
    "    user_hist = reviews[reviews[\"reviewer_id\"] == str(user_id)].copy().sort_values(\"date\")\n",
    "    recs = recommend_for_user(user_hist, feature_index, listing_embeddings, k=k)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"USER {user_id} | reviews={len(user_hist)} | unique_listings={user_hist['listing_id'].nunique()}\")\n",
    "    print(\"Last 3 reviewed listings:\", user_hist[\"listing_id\"].tail(3).tolist())\n",
    "    print(\"Top recommendations:\", recs[:k])\n",
    "\n",
    "    # show listing summaries\n",
    "    cols_to_show = [c for c in [\"name\",\"neighbourhood_cleansed\",\"room_type\",\"property_type\",\"accommodates\",\"price\"] if c in listings.columns]\n",
    "    if cols_to_show:\n",
    "        listings_indexed = listings.set_index(\"id\")\n",
    "        rev_df = listings_indexed.loc[user_hist['listing_id'], cols_to_show]\n",
    "        print(\"\\nReviewed listing details:\")\n",
    "        print(rev_df.to_string())\n",
    "\n",
    "        ordered_ids = [lid for lid in recs[:k] if lid in listings_indexed.index]\n",
    "        rec_df = listings_indexed.loc[ordered_ids, cols_to_show]\n",
    "        print(\"\\nRecommended listing details:\")\n",
    "        print(rec_df.to_string())\n",
    "\n",
    "# review samples\n",
    "sample_users = pick_returning_users(reviews, n=3, min_reviews=2)\n",
    "for uid in sample_users:\n",
    "    show_user_recs(uid, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527a228",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Baseline popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f214b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated users: 200\n",
      "recall@20: 0.0000\n",
      "recall@100: 0.0400\n",
      "recall@500: 0.0950\n",
      "recall@1000: 0.1600\n",
      "mrr@20: 0.0000\n",
      "mrr@100: 0.0008\n",
      "mrr@500: 0.0011\n",
      "mrr@1000: 0.0011\n"
     ]
    }
   ],
   "source": [
    "def popularity_baseline_eval(\n",
    "    reviews: pd.DataFrame,\n",
    "    feature_index: FeatureIndex,\n",
    "    n_users: int = 200,\n",
    "    min_unique: int = 2,\n",
    "    K_list=(5, 10, 20),\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Popularity baseline for next-item (last interaction) prediction.\n",
    "\n",
    "    For each user:\n",
    "      - sort by date\n",
    "      - hold out the last listing as target\n",
    "      - recommend most popular listings globally (from all reviews),\n",
    "        excluding the user's seen listings\n",
    "      - compute Recall@K and MRR@K\n",
    "    \"\"\"\n",
    "\n",
    "    df = reviews.copy()\n",
    "    df[\"reviewer_id\"] = df[\"reviewer_id\"].astype(str)\n",
    "    df[\"listing_id\"] = df[\"listing_id\"].astype(int)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"reviewer_id\", \"listing_id\", \"date\"])\n",
    "\n",
    "    # keep only listings that exist in our listing table / index\n",
    "    df = df[df[\"listing_id\"].isin(feature_index.lid2i)]\n",
    "\n",
    "    # global popularity ranking (most reviewed listings first)\n",
    "    pop_rank = df[\"listing_id\"].value_counts().index.to_numpy()\n",
    "\n",
    "    # pick eligible users\n",
    "    uniq_counts = df.groupby(\"reviewer_id\")[\"listing_id\"].nunique()\n",
    "    eligible = uniq_counts[uniq_counts >= min_unique].index.to_numpy()\n",
    "    if len(eligible) == 0:\n",
    "        raise ValueError(\"No eligible users found (min_unique too high).\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    chosen = rng.choice(eligible, size=min(n_users, len(eligible)), replace=False)\n",
    "\n",
    "    # metrics accumulators\n",
    "    metrics = {f\"recall@{k}\": 0.0 for k in K_list}\n",
    "    metrics.update({f\"mrr@{k}\": 0.0 for k in K_list})\n",
    "    n_eval = 0\n",
    "\n",
    "    for uid in chosen:\n",
    "        g = df[df[\"reviewer_id\"] == uid].sort_values(\"date\")\n",
    "        if g[\"listing_id\"].nunique() < min_unique:\n",
    "            continue\n",
    "\n",
    "        target = int(g[\"listing_id\"].iloc[-1])\n",
    "        hist = g.iloc[:-1]\n",
    "        if hist.empty:\n",
    "            continue\n",
    "\n",
    "        seen = set(hist[\"listing_id\"].tolist())\n",
    "\n",
    "        # recommend by popularity, excluding seen\n",
    "        recs = [lid for lid in pop_rank if lid not in seen]\n",
    "        if not recs:\n",
    "            continue\n",
    "\n",
    "        n_eval += 1\n",
    "        for k in K_list:\n",
    "            topk = recs[:k]\n",
    "            hit = 1.0 if target in topk else 0.0\n",
    "            metrics[f\"recall@{k}\"] += hit\n",
    "            if hit:\n",
    "                rank = topk.index(target) + 1\n",
    "                metrics[f\"mrr@{k}\"] += 1.0 / rank\n",
    "\n",
    "    # finalize\n",
    "    for k in K_list:\n",
    "        metrics[f\"recall@{k}\"] /= max(n_eval, 1)\n",
    "        metrics[f\"mrr@{k}\"] /= max(n_eval, 1)\n",
    "\n",
    "    return metrics, n_eval\n",
    "\n",
    "metrics, n_eval = popularity_baseline_eval(\n",
    "    reviews=reviews,\n",
    "    feature_index=feature_index,\n",
    "    n_users=200,\n",
    "    min_unique=2,\n",
    "    K_list=(20, 100, 500, 1000),\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Evaluated users: {n_eval}\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25f888",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dd9b6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 19:36:21 | INFO | recommender | === Evaluation start ===\n",
      "2026-02-26 19:37:04 | INFO | recommender | Evaluated users: 1000\n",
      "2026-02-26 19:37:04 | INFO | recommender | K=5 | Recall=0.0110 | MRR=0.0062\n",
      "2026-02-26 19:37:04 | INFO | recommender | K=10 | Recall=0.0160 | MRR=0.0069\n",
      "2026-02-26 19:37:04 | INFO | recommender | K=20 | Recall=0.0200 | MRR=0.0071\n",
      "2026-02-26 19:37:04 | INFO | recommender | === Evaluation finished ===\n"
     ]
    }
   ],
   "source": [
    "def evaluate_last_item_holdout(\n",
    "    reviews: pd.DataFrame,\n",
    "    feature_index: FeatureIndex,\n",
    "    listing_embeddings: torch.Tensor,\n",
    "    K_list=(5, 10, 20),\n",
    "    n_users: int | None = None,\n",
    "    logger: logging.Logger = LOGGER,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each user:\n",
    "      - Use all but last review as history\n",
    "      - Check if last reviewed listing is in top-K recommendations\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"=== Evaluation start ===\")\n",
    "\n",
    "    df = reviews.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"reviewer_id\", \"listing_id\", \"date\"])\n",
    "    df[\"reviewer_id\"] = df[\"reviewer_id\"].astype(str)\n",
    "    df[\"listing_id\"] = df[\"listing_id\"].astype(int)\n",
    "\n",
    "    # Only users with at least 2 distinct listings\n",
    "    user_counts = df.groupby(\"reviewer_id\")[\"listing_id\"].nunique()\n",
    "    eligible_users = user_counts[user_counts >= 2].index.tolist()\n",
    "\n",
    "    if n_users is not None:\n",
    "        eligible_users = eligible_users[:n_users]\n",
    "\n",
    "    max_k = max(K_list)\n",
    "\n",
    "    recall_scores = {k: 0.0 for k in K_list}\n",
    "    mrr_scores = {k: 0.0 for k in K_list}\n",
    "\n",
    "    n_evaluated = 0\n",
    "\n",
    "    for user_id in eligible_users:\n",
    "        user_df = df[df[\"reviewer_id\"] == user_id].sort_values(\"date\")\n",
    "\n",
    "        if user_df[\"listing_id\"].nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        target_listing = int(user_df[\"listing_id\"].iloc[-1])\n",
    "        history = user_df.iloc[:-1]\n",
    "\n",
    "        if target_listing not in feature_index.lid2i:\n",
    "            continue\n",
    "\n",
    "        recs = recommend_for_user(\n",
    "            history,\n",
    "            feature_index,\n",
    "            listing_embeddings,\n",
    "            k=max_k,\n",
    "        )\n",
    "\n",
    "        n_evaluated += 1\n",
    "\n",
    "        for k in K_list:\n",
    "            topk = recs[:k]\n",
    "            is_correct = target_listing in topk\n",
    "\n",
    "            recall_scores[k] += float(is_correct)\n",
    "\n",
    "            if is_correct:\n",
    "                rank_position = topk.index(target_listing) + 1\n",
    "                mrr_scores[k] += 1.0 / rank_position\n",
    "\n",
    "    if n_evaluated == 0:\n",
    "        logger.warning(\"No users evaluated.\")\n",
    "        return {}, 0\n",
    "\n",
    "    # Normalize\n",
    "    metrics = {}\n",
    "    for k in K_list:\n",
    "        recall = recall_scores[k] / n_evaluated\n",
    "        mrr = mrr_scores[k] / n_evaluated\n",
    "        metrics[f\"recall@{k}\"] = recall\n",
    "        metrics[f\"mrr@{k}\"] = mrr\n",
    "\n",
    "    logger.info(f\"Evaluated users: {n_evaluated}\")\n",
    "    for k in K_list:\n",
    "        logger.info(\n",
    "            f\"K={k} | Recall={metrics[f'recall@{k}']:.4f} | \"\n",
    "            f\"MRR={metrics[f'mrr@{k}']:.4f}\"\n",
    "        )\n",
    "\n",
    "    logger.info(\"=== Evaluation finished ===\")\n",
    "\n",
    "    return metrics, n_evaluated\n",
    "\n",
    "\n",
    "n_users = 1000\n",
    "metrics, n_eval = evaluate_last_item_holdout(\n",
    "    reviews=reviews,\n",
    "    feature_index=feature_index,\n",
    "    listing_embeddings=listing_embeddings,\n",
    "    n_users=n_users,\n",
    "    K_list=(5, 10, 20),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e4ef3",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
